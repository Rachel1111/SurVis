define({ entries : {
    "1": {
        "abstract": "This article discusses Nautilus(2022), a composition for solo bass flute created using machine-learning techniques and a Unity game engine. We consider the approaches we adopted and how they enhanced creativity and musicianship for those involved. We reflect on Unity's potential as a novel and flexible driver for the creation of a musical score in which traditional elements of compositional design are presented to a performer as a co-creator for interpretation and communication inside the act of musicking. Through this we offer insights into performer agency and how a performer decodes media, sound, images and AI through their instrument, personal skills and musical aesthetic. We describe how the notion of a music score was re-conceptualised, transforming our understanding of the activities of composition, collaboration and performance. </p>",
        "author": "Craig Vear and Carla Rees and Adam Stephenson",
        "doi": "10.1017/S0040298222000791",
        "issn": "0040-2982",
        "issue": "303",
        "journal": "Tempo",
        "month": "1",
        "pages": "33-42",
        "publisher": "Cambridge University Press",
        "series": "case study",
        "title": "NAUTILUS: A CASE STUDY IN HOW A DIGITAL SCORE CAN TRANSFORM CREATIVITY",
        "type": "article",
        "url": "https://www.cambridge.org/core/product/identifier/S0040298222000791/series/journal_article",
        "volume": "77",
        "year": "2023"
    },
    "10": {
        "abstract": "In this paper, we introduce new methods and discuss results of text-based LSTM (Long Short-Term Memory) networks for automatic music composition. The proposed network is designed to learn relationships within text documents that represent chord progressions and drum tracks in two case studies. In the experiments, word-RNNs (Recurrent Neural Networks) show good results for both cases, while character-based RNNs (char-RNNs) only succeed to learn chord progressions. The proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model.",
        "author": "Keunwoo Choi and George Fazekas and Mark Sandler",
        "month": "4",
        "series": "Qualitative or quantitative methods",
        "title": "Text-based LSTM networks for Automatic Music Composition",
        "type": "article",
        "url": "http://arxiv.org/abs/1604.05358",
        "year": "2016"
    },
    "2": {
        "abstract": "Generating music using Deep Neural Networks has been an active field of study for the past few decades. This paper discusses how to create computer-based monophonic musical content using a Recurrent Neural Network with the extension of Long Short-Term Memory. The objective of this study is to generate music that has a melodic nature to it and can be played by humans using sheet notation. The neural network is fed with input from the video game series: Final Fantasy for creating the melody. The selection of instruments is piano, which will be the same for the input and output. The main focus of this paper is on constrained music, differentiating notes from chords to help the architecture learn to distinguish one from the other and generate music with a mix of both, thus creating a pleasant listening experience.",
        "author": "Shikhar Bhardwaj and Shalmiya Mundeth Salim and Dr. Talha Ali Khan and Soudeh JavadiMasoudian",
        "doi": "10.1109/ICAI55857.2022.9960063",
        "isbn": "978-1-6654-7625-6",
        "journal": "2022 International Conference Automatics and Informatics (ICAI)",
        "month": "10",
        "pages": "193-198",
        "publisher": "IEEE",
        "series": "Qualitative or quantitative methods",
        "title": "Automated Music Generation using Deep Learning",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/document/9960063/",
        "year": "2022"
    },
    "3": {
        "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
        "author": "Zal\u00e1n Borsos and Rapha\u00ebl Marinier and Damien Vincent and Eugene Kharitonov and Olivier Pietquin and Matt Sharifi and Olivier Teboul and David Grangier and Marco Tagliasacchi and Neil Zeghidour",
        "month": "9",
        "series": "Qualitative or quantitative methods",
        "title": "AudioLM: a Language Modeling Approach to Audio Generation",
        "type": "article",
        "url": "http://arxiv.org/abs/2209.03143",
        "year": "2022"
    },
    "4": {
        "abstract": "This article discusses the creative and technical approaches in a performative robot project called \u201cEmbodied Musicking Robots\u201d (2018\u2013present). The core approach of this project is human-centered AI (HC-AI) which focuses on the design, development, and deployment of intelligent systems that cooperate with humans in real time in a \u201cdeep and meaningful way.\u201d This project applies this goal as a central philosophy from which the concepts of creative AI and experiential learning are developed. At the center of this discussion is the articulation of a shift in thinking of what constitutes creative AI and new HC-AI forms of computational learning from inside the flow of the shared experience between robots and humans. The central case study (EMRv1) investigates the technical solutions and artistic potential of AI-driven robots co-creating with an improvising human musician (the author) in real time. This project is ongoing, currently at v4, with limited conclusions; other than this, the approach can be felt to be cooperative but requires further investigation.",
        "author": "Craig Vear",
        "doi": "10.3389/frobt.2021.631752",
        "issn": "2296-9144",
        "journal": "Frontiers in Robotics and AI",
        "series": "case study",
        "title": "Creative AI and Musicking Robots",
        "type": "article",
        "url": "https://www.frontiersin.org/articles/10.3389/frobt.2021.631752",
        "volume": "8",
        "year": "2021"
    },
    "5": {
        "abstract": "Generating a complex work of art such as a musical composition requires exhibiting true creativity that depends on a variety of factors that are related to the hierarchy of musical language. Music generation have been faced with Algorithmic methods and recently, with Deep Learning models that are being used in other fields such as Computer Vision. In this paper we want to put into context the existing relationships between AI-based music composition models and human musical composition and creativity processes. We give an overview of the recent Deep Learning models for music composition and we compare these models to the music composition process from a theoretical point of view. We have tried to answer some of the most relevant open questions for this task by analyzing the ability of current Deep Learning models to generate music with creativity or the similarity between AI and human composition processes, among others.",
        "author": "Carlos Hernandez-Olivan and Jose R. Beltran",
        "month": "8",
        "series": "case study",
        "title": "Music Composition with Deep Learning: A Review",
        "type": "article",
        "url": "http://arxiv.org/abs/2108.12290",
        "year": "2021"
    },
    "6": {
        "abstract": "Singing voice synthesis (SVS) systems are built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing. In this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain that iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generate realistic outputs. To further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we propose boundary prediction methods to locate the intersection and determine the shallow step adaptively. The evaluations conducted on a Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Extensional experiments also prove the generalization of our methods on text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io. Codes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this work: \"Diffsinger: Diffusion acoustic model for singing voice synthesis\".",
        "author": "Jinglin Liu and Chengxi Li and Yi Ren and Feiyang Chen and Zhou Zhao",
        "month": "5",
        "series": "experiment",
        "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism",
        "type": "article",
        "url": "http://arxiv.org/abs/2105.02446",
        "year": "2021"
    },
    "7": {
        "abstract": "Realistic music generation has always remained as a challenging problem as it may lack structure or rationality. In this work, we propose a deep learning based music generation method in order to produce old style music particularly JAZZ with rehashed melodic structures utilizing a Bi-directional Long Short Term Memory (Bi-LSTM) Neural Network with Attention. Owing to the success in modelling long-term temporal dependencies in sequential data and its success in case of videos, Bi-LSTMs with attention serve as the natural choice and early utilization in music generation. We validate in our experiments that Bi-LSTMs with attention are able to preserve the richness and technical nuances of the music performed.",
        "author": "Gullapalli Keerti and A N Vaishnavi and Prerana Mukherjee and A Sree Vidya and Gattineni Sai Sreenithya and Deeksha Nayab",
        "month": "2",
        "series": "experiment",
        "title": "Attentional networks for music generation",
        "type": "article",
        "url": "http://arxiv.org/abs/2002.03854",
        "year": "2020"
    },
    "8": {
        "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
        "author": "Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Noam Shazeer and Ian Simon and Curtis Hawthorne and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck",
        "month": "9",
        "series": "experiment",
        "title": "Music Transformer",
        "type": "article",
        "url": "http://arxiv.org/abs/1809.04281",
        "year": "2018"
    },
    "9": {
        "abstract": "This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.",
        "author": "Ga\u00ebtan Hadjeres and Fran\u00e7ois Pachet and Frank Nielsen",
        "month": "12",
        "series": "Qualitative or quantitative methods",
        "title": "DeepBach: a Steerable Model for Bach Chorales Generation",
        "type": "article",
        "url": "http://arxiv.org/abs/1612.01010",
        "year": "2016"
    }
}});